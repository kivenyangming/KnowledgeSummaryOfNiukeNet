● SGD,Momentum,Adagard,Adam原理\
参考回答：\
SGD为随机梯度下降,每一次迭代计算数据集的mini-batch的梯度,然后对参数进行跟新。\
Momentum参考了物理中动量的概念,前几次的梯度也会参与到当前的计算中,但是前几轮的梯度叠加在当前计算中会有一定的衰减。\
Adagard在训练的过程中可以自动变更学习的速率,设置一个全局的学习率,而实际的学习率与以往的参数模和的开方成反比。\
Adam利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率,在经过偏置的校正后,每一次迭代后的学习率都有个确定的范围,使得参数较为平稳。

● L1不可导的时候该怎么办\
参考回答：\
当损失函数不可导,梯度下降不再有效,可以使用坐标轴下降法,梯度下降是沿着当前点的负梯度方向进行参数更新,而坐标轴下降法是沿着坐标轴的方向,假设有m个特征个数,坐标轴下降法进参数更新的时候,先固定m-1个值,然后再求另外一个的局部最优解,从而避免损失函数不可导问题。\
使用Proximal Algorithm对L1进行求解,此方法是去优化损失函数上界结果。

1. SGD 梯度下降法
1.1 梯度下降（Gradient Descent）
梯度g指函数的某处的偏导数，指向函数上升方向。因此梯度下降法是指用梯度的负数-g更新参数，从而使下一次计算的结果向函数下降方向逼近，从而得到最小值。其中更新时乘的系数称为学习率。


1.2 批次梯度下降（Batch Gradient Descent）
以所有m个数据作为一个批次，每次计算损失loss值和梯度g（偏导）时为所有数据的累加和，更新每个参数时也都是以所有数据的梯度累加和进行计算更新。
优点：下降方向为全局最优值 缺点：计算所有数据的梯度非常耗时

1.3 随机梯度下降（Stochastic Gradient Desent, SGD）
虽然m个数据为一个批次，但是更新参数时仅使用随机一个数据的梯度进行更新。
优点：很快 缺点：随机性高，噪声影响严重，不一定向整体最优点下降。

1.4 小批次梯度下降 Mini-batch GD(MBGD)
把所有样本分为n个batch（一般是随机的），每次计算损失和梯度时用一个batch的数据进行计算，并更新参数，从而避免了唯一随机性和全局计算的耗时性。
优点：得到的梯度下降方向是局部最优的，整体速度快。

1.6 一般说的 SGD 其实就指的是 Mini-batch GD


2. 动量梯度下降 Gradient Dscent with Momentum
梯度下降法可能会停滞到 平原、鞍点和局部最优点（在这三个点梯度均为0），因此带动量的梯度下降法能依靠之前的梯度值，“冲过平原、鞍点和局部最优点”，提高泛化性。


 表示历史参数的更新值的大小，相当于现在和过去的梯度的一个加权，可以视为当前的一个动量（势能），每次计算新一次的更新值时考虑当前梯度和历史更新值，其实就是用一阶梯度的指数移动平均代替梯度值进行参数更新如下：


3. 自适应梯度算法 Adagard（Adaptive gradient)
Adagard 针对不同的变量提供不同的学习率。 当一些变量被优化到最优点，但另外一些变量没到最优点，使用统一的学习率就会影响优化过程，太大或太小都不合适。太大不容易收敛，太小收敛缓慢。
解决方式：为每一参数建立历史累计梯度值，利用历史累计梯度作为分母，从而使各个参数在训练后期被给予不同的除数，得到自适应参数值。


4. RMSprop自适应学习率算法（root mean square propagation）
Adagard 暴力累加参数之前的所有梯度平方作为分母进行自适应（二阶梯度的梯度下降？），而RMSprop进行历史梯度平方和的加权；
用来控制衰减程度（通常为0.9），每次不再直接累加，而是一个指数移动平均，即是用二阶梯度的移动平均代替当前梯度进行更新参数。


5. Adam优化器 （Adaptive moment estimation）
Adam 可以看做 RMSprop 与 Momentum 的结合，使用了一阶梯度的指数移动平均（Momentum)和二阶梯度的指数移动平均（RMSprop）。
优点:每一次迭代学习率都有一个明确的范围,使得参数变化很平稳.
注意到，在迭代初始阶段， 和  有一个向初值的偏移（过多的偏向了 0）。因此，可以对一阶和二阶动量做偏置校正 (bias correction)，

● 求m*k矩阵A和n*k矩阵的欧几里得距离?\
参考回答：\
先得到矩阵,然后对矩阵A和矩阵分别求出其中每个向量的模平方,并扩展为两个m*k的矩阵和。最终求得新的矩阵,并将此矩阵开平方得到A,B向量集的欧几里得距离。\

● 矩阵正定性的判断,Hessian矩阵正定性在梯度下降中的应用\
参考回答：\
若矩阵所有特征值均不小于0,则判定为半正定。若矩阵所有特征值均大于0,则判定为正定。在判断优化算法的可行性时Hessian矩阵的正定性起到了很大的作用,若Hessian正定,则函数的二阶偏导恒大于0,函数的变化率处于递增状态,在牛顿法等梯度下降的方法中,Hessian矩阵的正定性可以很容易的判断函数是否可收敛到局部或全局最优解。\
● 概率题：抽蓝球红球，蓝结束红放回继续，平均结束游戏抽取次数\
参考回答：\
根据红球和蓝球的个数，依据概率公式，求出平均抽取次数。\
● 讲一下PCA\
参考回答：\
PCA是比较常见的线性降维方法,通过线性投影将高维数据映射到低维数据中,所期望的是在投影的维度上,新特征自身的方差尽量大,方差越大特征越有效,尽量使产生的新特征间的相关性越小。\
PCA算法的具体操作为对所有的样本进行中心化操作,计算样本的协方差矩阵,然后对协方差矩阵做特征值分解,取最大的n个特征值对应的特征向量构造投影矩阵。

● 拟牛顿法的原理\
参考回答：\
牛顿法的收敛速度快,迭代次数少,但是Hessian矩阵很稠密时,每次迭代的计算量很大,随着数据规模增大,Hessian矩阵也会变大,需要更多的存储空间以及计算量。拟牛顿法就是在牛顿法的基础上引入了Hessian矩阵的近似矩阵,避免了每次都计算Hessian矩阵的逆,在拟牛顿法中,用Hessian矩阵的逆矩阵来代替Hessian矩阵,虽然不能像牛顿法那样保证最优化的方向,但其逆矩阵始终是正定的,因此算法始终朝最优化的方向搜索。\
● 编辑距离\
参考回答：\
概念
编辑距离的作用主要是用来比较两个字符串的相似度的\
编辑距离，又称Levenshtein距离（莱文斯坦距离也叫做Edit Distance），是指两个字串之间，由一个转成另一个所需的最少编辑操作次数，如果它们的距离越大，说明它们越是不同。许可的编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符。\
在概念中，我们可以看出一些重点那就是，编辑操作只有三种。插入，删除，替换这三种操作，我们有两个字符串，将其中一个字符串经过上面的这三种操作之后，得到两个完全相同的字符串付出的代价是什么就是我们要讨论和计算的。\
